version: '3.8'
services:
  spark-master:
    build:
      context: .
      dockerfile: docker/spark-base.Dockerfile
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - WAREHOUSE_URI=/lake/warehouse
    ports:
      - "7077:7077"
      - "8080:8080"
      - "6066:6066"
      - "18080:18080"
    volumes:
      - ./lake:/lake
      - ./conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./conf/log4j2.properties:/opt/spark/conf/log4j2.properties
      - ./conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml

  spark-worker:
    build:
      context: .
      dockerfile: docker/spark-base.Dockerfile
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - WAREHOUSE_URI=/lake/warehouse
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    volumes:
      - ./lake:/lake

  streamlit:
    build:
      context: .
      dockerfile: docker/streamlit.Dockerfile
    command: streamlit run apps/streamlit_app.py --server.port=8501 --server.address=0.0.0.0
    environment:
      - MAP_USE_BIGQUERY=0
      - WAREHOUSE_URI=/lake/warehouse
    ports:
      - "8501:8501"
    volumes:
      - .:/app
      - ./lake:/lake
    depends_on:
      - spark-master

  lightdash:
    build:
      context: .
      dockerfile: docker/lightdash.Dockerfile
    ports:
      - "8085:8080"
    environment:
      - LIGHTDASH_LOG_LEVEL=info
    volumes:
      - .:/project
      - ./dbt:/project/dbt
